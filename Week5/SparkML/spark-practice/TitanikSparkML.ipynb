{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/aleksandrsavchenko/Library/Caches/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471/pyspark-2.4.4-py2.py3-none-any.whl\n",
      "Collecting py4j==0.10.7\n",
      "  Using cached https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PySparkTitanikJob\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.147:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkTitanikJob</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11602fa10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = spark.read.csv('train.csv', header = 'True', inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Cabin: string, Embarked: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "passengers_count = titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gropuBy_output = titanic_df.groupBy(\"Survived\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gropuBy_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_age = round(df.select(avg(col('Age'))).collect()[0][0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.fillna({'Age': avg_age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|30.0|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|30.0|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|30.0|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.fillna({\"Embarked\" : 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"Cabin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn('Alone',lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.withColumn(\"Alone\",when(titanic_df[\"Family_Size\"] == 0, 1).otherwise(titanic_df[\"Alone\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+--------+-----------+-----+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|   Ticket|   Fare|Embarked|Family_Size|Alone|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+--------+-----------+-----+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|A/5 21171|   7.25|       S|          1|    0|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0| PC 17599|71.2833|       C|          1|    0|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+---------+-------+--------+-----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(titanic_df) for column in [\"Sex\",\"Embarked\"]]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "titanic_df = pipeline.fit(titanic_df).transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = titanic_df.drop(\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|30.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|\n",
      "|       1|     2|30.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|\n",
      "|       1|     3|30.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = VectorAssembler(inputCols=titanic_df.columns[1:],outputCol=\"features\")\n",
    "feature_vector= feature.transform(titanic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|   Fare|Family_Size|Alone|Sex_index|Embarked_index|            features|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n",
      "|       0|     3|22.0|    1|    0|   7.25|          1|    0|      0.0|           0.0|[3.0,22.0,1.0,0.0...|\n",
      "|       1|     1|38.0|    1|    0|71.2833|          1|    0|      1.0|           1.0|[1.0,38.0,1.0,0.0...|\n",
      "|       1|     3|26.0|    0|    0|  7.925|          0|    1|      1.0|           0.0|[3.0,26.0,0.0,0.0...|\n",
      "|       1|     1|35.0|    1|    0|   53.1|          1|    0|      1.0|           0.0|[1.0,35.0,1.0,0.0...|\n",
      "|       0|     3|35.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[3.0...|\n",
      "|       0|     3|30.0|    0|    0| 8.4583|          0|    1|      0.0|           2.0|[3.0,30.0,0.0,0.0...|\n",
      "|       0|     1|54.0|    0|    0|51.8625|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     3| 2.0|    3|    1| 21.075|          4|    0|      0.0|           0.0|[3.0,2.0,3.0,1.0,...|\n",
      "|       1|     3|27.0|    0|    2|11.1333|          2|    0|      1.0|           0.0|[3.0,27.0,0.0,2.0...|\n",
      "|       1|     2|14.0|    1|    0|30.0708|          1|    0|      1.0|           1.0|[2.0,14.0,1.0,0.0...|\n",
      "|       1|     3| 4.0|    1|    1|   16.7|          2|    0|      1.0|           0.0|[3.0,4.0,1.0,1.0,...|\n",
      "|       1|     1|58.0|    0|    0|  26.55|          0|    1|      1.0|           0.0|[1.0,58.0,0.0,0.0...|\n",
      "|       0|     3|20.0|    0|    0|   8.05|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[3.0...|\n",
      "|       0|     3|39.0|    1|    5| 31.275|          6|    0|      0.0|           0.0|[3.0,39.0,1.0,5.0...|\n",
      "|       0|     3|14.0|    0|    0| 7.8542|          0|    1|      1.0|           0.0|[3.0,14.0,0.0,0.0...|\n",
      "|       1|     2|55.0|    0|    0|   16.0|          0|    1|      1.0|           0.0|[2.0,55.0,0.0,0.0...|\n",
      "|       0|     3| 2.0|    4|    1| 29.125|          5|    0|      0.0|           2.0|[3.0,2.0,4.0,1.0,...|\n",
      "|       1|     2|30.0|    0|    0|   13.0|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[2.0...|\n",
      "|       0|     3|31.0|    1|    0|   18.0|          1|    0|      1.0|           0.0|[3.0,31.0,1.0,0.0...|\n",
      "|       1|     3|30.0|    0|    0|  7.225|          0|    1|      1.0|           1.0|[3.0,30.0,0.0,0.0...|\n",
      "+--------+------+----+-----+-----+-------+-----------+-----+---------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = feature_vector.randomSplit([0.8, 0.2],seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|Survived|Pclass| Age|SibSp|Parch|    Fare|Family_Size|Alone|Sex_index|Embarked_index|            features|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+--------------------+\n",
      "|       0|     1| 2.0|    1|    2|  151.55|          3|    0|      1.0|           0.0|[1.0,2.0,1.0,2.0,...|\n",
      "|       0|     1|21.0|    0|    1| 77.2875|          1|    0|      0.0|           0.0|[1.0,21.0,0.0,1.0...|\n",
      "|       0|     1|22.0|    0|    0|135.6333|          0|    1|      0.0|           1.0|[1.0,22.0,0.0,0.0...|\n",
      "|       0|     1|24.0|    0|    0|    79.2|          0|    1|      0.0|           1.0|[1.0,24.0,0.0,0.0...|\n",
      "|       0|     1|24.0|    0|    1|247.5208|          1|    0|      0.0|           1.0|[1.0,24.0,0.0,1.0...|\n",
      "|       0|     1|25.0|    1|    2|  151.55|          3|    0|      1.0|           0.0|[1.0,25.0,1.0,2.0...|\n",
      "|       0|     1|27.0|    0|    2|   211.5|          2|    0|      0.0|           1.0|[1.0,27.0,0.0,2.0...|\n",
      "|       0|     1|28.0|    0|    0|    47.1|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     1|28.0|    1|    0| 82.1708|          1|    0|      0.0|           1.0|[1.0,28.0,1.0,0.0...|\n",
      "|       0|     1|29.0|    0|    0|    30.0|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     1|30.0|    0|    0|     0.0|          0|    1|      0.0|           0.0|(9,[0,1,6],[1.0,3...|\n",
      "|       0|     1|30.0|    0|    0|     0.0|          0|    1|      0.0|           0.0|(9,[0,1,6],[1.0,3...|\n",
      "|       0|     1|30.0|    0|    0|    26.0|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     1|30.0|    0|    0|   26.55|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     1|30.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|[1.0,30.0,0.0,0.0...|\n",
      "|       0|     1|30.0|    0|    0| 27.7208|          0|    1|      0.0|           1.0|[1.0,30.0,0.0,0.0...|\n",
      "|       0|     1|30.0|    0|    0|   27.75|          0|    1|      0.0|           1.0|[1.0,30.0,0.0,0.0...|\n",
      "|       0|     1|30.0|    0|    0| 30.6958|          0|    1|      0.0|           1.0|[1.0,30.0,0.0,0.0...|\n",
      "|       0|     1|30.0|    0|    0|    31.0|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "|       0|     1|30.0|    0|    0|    35.0|          0|    1|      0.0|           0.0|(9,[0,1,4,6],[1.0...|\n",
      "+--------+------+----+-----+-----+--------+-----------+-----+---------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,18.0,1.0,0.0...|\n",
      "|       1.0|       0|[1.0,19.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       1.0|       0|[1.0,29.0,1.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "#Training algo\n",
    "lrModel = lr.fit(training_data)\n",
    "lr_prediction = lrModel.transform(test_data)\n",
    "lr_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression [Accuracy] = 0.793103\n",
      "LogisticRegression [Error] = 0.206897 \n"
     ]
    }
   ],
   "source": [
    "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
    "print(\"LogisticRegression [Accuracy] = %g\"% (lr_accuracy))\n",
    "print(\"LogisticRegression [Error] = %g \" % (1.0 - lr_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,18.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       0.0|       0|[1.0,29.0,1.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(training_data)\n",
    "dt_prediction = dt_model.transform(test_data)\n",
    "\n",
    "dt_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier [Accuracy] = 0.798851\n",
      "DecisionTreeClassifier [Error] = 0.201149 \n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
    "print(\"DecisionTreeClassifier [Accuracy] = %g\"% (dt_accuracy))\n",
    "print(\"DecisionTreeClassifier [Error] = %g \" % (1.0 - dt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,18.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       0.0|       0|[1.0,29.0,1.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
    "rf_model = rf.fit(training_data)\n",
    "rf_prediction = rf_model.transform(test_data)\n",
    "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier [Accuracy] = 0.827586\n",
      "RandomForestClassifier [Error] = 0.172414\n"
     ]
    }
   ],
   "source": [
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"RandomForestClassifier [Accuracy] = %g\"% (rf_accuracy))\n",
    "print(\"RandomForestClassifier [Error] = %g\" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-boosted tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       0.0|       0|[1.0,18.0,1.0,0.0...|\n",
      "|       1.0|       0|[1.0,19.0,1.0,0.0...|\n",
      "|       0.0|       0|[1.0,19.0,3.0,2.0...|\n",
      "|       1.0|       0|[1.0,29.0,1.0,0.0...|\n",
      "|       0.0|       0|(9,[0,1,4,6],[1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxIter=10)\n",
    "gbt_model = gbt.fit(training_data)\n",
    "gbt_prediction = gbt_model.transform(test_data)\n",
    "gbt_prediction.select(\"prediction\", \"Survived\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient-boosted [Accuracy] = 0.793103\n",
      "Gradient-boosted [Error] = 0.206897\n"
     ]
    }
   ],
   "source": [
    "gbt_accuracy = evaluator.evaluate(gbt_prediction)\n",
    "print(\"Gradient-boosted [Accuracy] = %g\"% (gbt_accuracy))\n",
    "print(\"Gradient-boosted [Error] = %g\"% (1.0 - gbt_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.save('rf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.RandomForestClassificationModel"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "type(RandomForestClassificationModel.load('rf_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
